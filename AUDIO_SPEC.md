# Voice WebSocket í”„ë¡œí† ì½œ ê¸°ìˆ  ë¬¸ì„œ

## ê°œìš”

ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™” ì‹œìŠ¤í…œì˜ WebSocket í”„ë¡œí† ì½œ ëª…ì„¸ì…ë‹ˆë‹¤.
ìŒì„± ì…ë ¥ â†’ STT â†’ LLM â†’ TTS íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

**ë™ì‘ ë°©ì‹:** Full-duplex ì‹¤ì‹œê°„ ëŒ€í™”
- STT ì—°ê²°ì´ ì§€ì†ì ìœ¼ë¡œ ìœ ì§€ë˜ì–´ ì‚¬ìš©ì ìŒì„±ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ì‚¬
- AssemblyAIì˜ `format_turns` ê¸°ëŠ¥ìœ¼ë¡œ ë°œí™” ì¢…ë£Œë¥¼ ìë™ ê°ì§€
- ë°œí™” ì¢…ë£Œ ê°ì§€ ì‹œ ìë™ìœ¼ë¡œ LLM ì‘ë‹µ ìƒì„±
- AI ì‘ë‹µ ì¤‘ ì‚¬ìš©ìê°€ ë§í•˜ë©´ ìë™ìœ¼ë¡œ ì¤‘ë‹¨(interrupt)

---

## 1. ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         í´ë¼ì´ì–¸íŠ¸ (Web)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ë§ˆì´í¬ í•­ìƒ í™œì„± â†’ PCM 16kHz â†’ WebSocket ì „ì†¡                    â”‚
â”‚  STT ì‹¤ì‹œê°„ í”¼ë“œë°± í‘œì‹œ                                           â”‚
â”‚  ë°œí™” ì¢…ë£Œ ìë™ ê°ì§€ â†’ LLM ì‘ë‹µ ìë™ ìƒì„±                          â”‚
â”‚  AI ì‘ë‹µ ì¤‘ ì‚¬ìš©ì ë°œí™” ì‹œ ìë™ ì¤‘ë‹¨                               â”‚
â”‚  <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ WebSocket â†’ JSON ì´ë²¤íŠ¸ â†’ ì˜¤ë””ì˜¤ ì¬ìƒ       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â”‚ ws:// ë˜ëŠ” wss://
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            ì„œë²„ (Rails)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ì§€ì†ì  STT ì—°ê²° â†’ ë°œí™” ì¢…ë£Œ ê°ì§€ â†’ LLM â†’ TTS                     â”‚
â”‚  Interrupt ì²˜ë¦¬ë¡œ TTS ì¤‘ë‹¨ ì§€ì›                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. WebSocket ì—°ê²°

**ì—”ë“œí¬ì¸íŠ¸:** `/ws`

```
ws://{host}/ws
wss://{host}/ws  (HTTPS)
```

> **Note:** Railsì—ì„œ raw WebSocketì„ ì‚¬ìš©í•©ë‹ˆë‹¤ (Action Cableì´ ì•„ë‹˜).

---

## 3. ë©”ì‹œì§€ í”„ë¡œí† ì½œ

### 3.1 í´ë¼ì´ì–¸íŠ¸ â†’ ì„œë²„

| í˜•ì‹ | íƒ€ì… | ì„¤ëª… |
|------|------|------|
| Binary | `ArrayBuffer` | PCM ì˜¤ë””ì˜¤ (16kHz, 16-bit signed LE, mono) |
| JSON | `{ type: "auto_end_of_speech" }` | ë°œí™” ì¢…ë£Œ ê°ì§€ í›„ LLM ì²˜ë¦¬ ìš”ì²­ |
| JSON | `{ type: "interrupt" }` | AI ì‘ë‹µ ì¤‘ë‹¨ ìš”ì²­ |

**ì˜¤ë””ì˜¤ ì²­í¬ í¬ê¸°:** 1,600 ìƒ˜í”Œ (100ms)

### 3.2 ì„œë²„ â†’ í´ë¼ì´ì–¸íŠ¸

ëª¨ë“  ì´ë²¤íŠ¸ëŠ” JSON ë¬¸ìì—´ë¡œ ì „ì†¡ë©ë‹ˆë‹¤.

---

## 4. ì´ë²¤íŠ¸ íƒ€ì…

### ê³µí†µ í•„ë“œ

| í•„ë“œ | íƒ€ì… | ì„¤ëª… |
|------|------|------|
| `type` | `string` | ì´ë²¤íŠ¸ íƒ€ì… |
| `ts` | `number` | íƒ€ì„ìŠ¤íƒ¬í”„ (ms, Unix epoch) |

### 4.1 í´ë¼ì´ì–¸íŠ¸ â†’ ì„œë²„ ì´ë²¤íŠ¸

#### `auto_end_of_speech` - ë°œí™” ì¢…ë£Œ í›„ LLM ì²˜ë¦¬

AssemblyAIê°€ ë°œí™” ì¢…ë£Œë¥¼ ê°ì§€í•˜ì—¬ `stt_output` ì´ë²¤íŠ¸ë¥¼ ë³´ë‚¸ í›„, í´ë¼ì´ì–¸íŠ¸ê°€ ìë™ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.

```json
{ "type": "auto_end_of_speech" }
```

#### `interrupt` - AI ì‘ë‹µ ì¤‘ë‹¨

ì‚¬ìš©ìê°€ AI ì‘ë‹µ ì¤‘ì— ë§ì„ ì‹œì‘í•  ë•Œ ì „ì†¡í•˜ì—¬ TTSë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.

```json
{ "type": "interrupt" }
```

### 4.2 ì„œë²„ â†’ í´ë¼ì´ì–¸íŠ¸ ì´ë²¤íŠ¸

#### `stt_chunk` - ë¶€ë¶„ ì „ì‚¬ (ì‹¤ì‹œê°„)

```json
{ "type": "stt_chunk", "ts": 1704355200000, "transcript": "ì•ˆë…•í•˜ì„¸" }
```

#### `stt_output` - ìµœì¢… ì „ì‚¬ (ë°œí™” ì¢…ë£Œ ê°ì§€)

AssemblyAIì˜ `format_turns` ê¸°ëŠ¥ì´ ë°œí™” ì¢…ë£Œë¥¼ ê°ì§€í•˜ë©´ ì „ì†¡ë©ë‹ˆë‹¤.

```json
{ "type": "stt_output", "ts": 1704355201000, "transcript": "ì•ˆë…•í•˜ì„¸ìš”." }
```

#### `llm_chunk` - LLM ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°

```json
{ "type": "llm_chunk", "ts": 1704355202000, "text": "ì•ˆë…•í•˜ì„¸ìš”!" }
```

#### `llm_end` - LLM ì‘ë‹µ ì™„ë£Œ

```json
{ "type": "llm_end", "ts": 1704355203000 }
```

#### `tts_chunk` - ìŒì„± í•©ì„± ì˜¤ë””ì˜¤

```json
{
  "type": "tts_chunk",
  "ts": 1704355204000,
  "audio": "base64...",
  "tts_generation": 1
}
```

**TTS ì˜¤ë””ì˜¤ í¬ë§·:** 24kHz, 16-bit signed little-endian, mono, Base64 ì¸ì½”ë”©
**tts_generation:** TTS ìƒì„± ë²ˆí˜¸. interrupt ì‹œ ì¦ê°€í•˜ì—¬ ì´ì „ ì²­í¬ ë¬´ì‹œ

#### `tts_end` - TTS ì¬ìƒ ì™„ë£Œ

```json
{ "type": "tts_end", "ts": 1704355205000 }
```

#### `interrupted` - ì¤‘ë‹¨ í™•ì¸

```json
{
  "type": "interrupted",
  "ts": 1704355206000,
  "tts_generation": 2
}
```

#### `error` - ì—ëŸ¬

```json
{ "type": "error", "ts": 1704355207000, "message": "STT connection failed" }
```

---

## 5. ì´ë²¤íŠ¸ íë¦„ (ì‹¤ì‹œê°„ ëŒ€í™”)

```
í´ë¼ì´ì–¸íŠ¸                         ì„œë²„
    â”‚                               â”‚
    â”‚  [WebSocket ì—°ê²°]              â”‚
    â”‚<â”€â”€ llm_chunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  (ì´ˆê¸° ì¸ì‚¬ë§)
    â”‚<â”€â”€ llm_end â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
    â”‚<â”€â”€ tts_chunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
    â”‚                               â”‚
    â”‚  [ë§ˆì´í¬ í•­ìƒ í™œì„±]             â”‚
    â”‚â”€â”€ Binary (PCM) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  (ì§€ì†ì  ì „ì†¡)
    â”‚<â”€â”€ stt_chunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  (ì‹¤ì‹œê°„ í”¼ë“œë°±)
    â”‚â”€â”€ Binary (PCM) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
    â”‚<â”€â”€ stt_chunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
    â”‚                               â”‚
    â”‚  [ë°œí™” ì¢…ë£Œ ìë™ ê°ì§€]           â”‚
    â”‚<â”€â”€ stt_output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  (format_turns ê°ì§€)
    â”‚â”€â”€ { type: "auto_end_of_speech" } â”€>â”‚
    â”‚                               â”‚  LLM ì²˜ë¦¬
    â”‚<â”€â”€ llm_chunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
    â”‚<â”€â”€ llm_chunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
    â”‚<â”€â”€ llm_end â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
    â”‚                               â”‚  TTS ì²˜ë¦¬
    â”‚<â”€â”€ tts_chunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  (generation: 1)
    â”‚<â”€â”€ tts_chunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
    â”‚                               â”‚
    â”‚  [AI ì‘ë‹µ ì¤‘ ì‚¬ìš©ì ë°œí™”]        â”‚
    â”‚â”€â”€ { type: "interrupt" } â”€â”€â”€â”€â”€>â”‚
    â”‚<â”€â”€ interrupted â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  (generation: 2)
    â”‚<â”€â”€ stt_chunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  (ìƒˆ ë°œí™” ì „ì‚¬)
    â”‚   ...                         â”‚
```

---

## 6. íƒ€ì… ì •ì˜

### TypeScript (í´ë¼ì´ì–¸íŠ¸)

```typescript
// í´ë¼ì´ì–¸íŠ¸ â†’ ì„œë²„
type ClientEvent =
  | { type: "auto_end_of_speech" }
  | { type: "interrupt" };

// ì„œë²„ â†’ í´ë¼ì´ì–¸íŠ¸
type ServerEvent =
  | { type: "stt_chunk"; ts: number; transcript: string }
  | { type: "stt_output"; ts: number; transcript: string }
  | { type: "llm_chunk"; ts: number; text: string }
  | { type: "llm_end"; ts: number }
  | { type: "tts_chunk"; ts: number; audio: string; tts_generation?: number }
  | { type: "tts_end"; ts: number }
  | { type: "interrupted"; ts: number; tts_generation: number }
  | { type: "error"; ts: number; message: string };
```

### Ruby (ì„œë²„)

```ruby
def emit(type, **data)
  { type: type, ts: (Time.now.to_f * 1000).to_i, **data }
end
```

---

## 7. í”„ë¡ íŠ¸ì—”ë“œ êµ¬í˜„ ê°€ì´ë“œ

### 7.1 ì˜¤ë””ì˜¤ ìº¡ì²˜ (ë§ˆì´í¬ â†’ PCM 16kHz)

ë§ˆì´í¬ë¥¼ í•­ìƒ í™œì„±í™”í•˜ì—¬ ì§€ì†ì ìœ¼ë¡œ ì˜¤ë””ì˜¤ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.

```typescript
const workletCode = `
class PCMProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    this.buffer = [];
    this.resampleRatio = sampleRate / 16000;
    this.resampleIndex = 0;
  }

  process(inputs) {
    const input = inputs[0]?.[0];
    if (!input) return true;

    for (let i = 0; i < input.length; i++) {
      this.resampleIndex += 1;
      if (this.resampleIndex >= this.resampleRatio) {
        this.resampleIndex -= this.resampleRatio;
        const sample = Math.max(-1, Math.min(1, input[i]));
        const int16 = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
        this.buffer.push(int16);
      }
    }

    const CHUNK_SIZE = 1600;
    while (this.buffer.length >= CHUNK_SIZE) {
      const chunk = this.buffer.splice(0, CHUNK_SIZE);
      const int16Array = new Int16Array(chunk);
      this.port.postMessage(int16Array.buffer, [int16Array.buffer]);
    }
    return true;
  }
}
registerProcessor('pcm-processor', PCMProcessor);
`;

export function createAudioCapture() {
  let audioContext: AudioContext | null = null;
  let workletNode: AudioWorkletNode | null = null;
  let mediaStream: MediaStream | null = null;

  return {
    async start(onChunk: (data: ArrayBuffer) => void) {
      audioContext = new AudioContext();
      const blob = new Blob([workletCode], { type: 'application/javascript' });
      const url = URL.createObjectURL(blob);
      await audioContext.audioWorklet.addModule(url);
      URL.revokeObjectURL(url);

      mediaStream = await navigator.mediaDevices.getUserMedia({
        audio: { echoCancellation: true, noiseSuppression: true, autoGainControl: true }
      });

      const source = audioContext.createMediaStreamSource(mediaStream);
      workletNode = new AudioWorkletNode(audioContext, 'pcm-processor');
      workletNode.port.onmessage = (e) => onChunk(e.data);
      source.connect(workletNode);
    },

    stop() {
      workletNode?.disconnect();
      mediaStream?.getTracks().forEach(t => t.stop());
      workletNode = null;
      mediaStream = null;
    }
  };
}
```

### 7.2 ì˜¤ë””ì˜¤ ì¬ìƒ (Base64 PCM â†’ ìŠ¤í”¼ì»¤)

TTS ìƒì„± ë²ˆí˜¸ë¥¼ ì¶”ì í•˜ì—¬ interrupt ì‹œ ì´ì „ ì²­í¬ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.

```typescript
const TTS_SAMPLE_RATE = 24000;

export function createAudioPlayback() {
  let audioContext: AudioContext | null = null;
  let nextPlayTime = 0;
  let currentGeneration = 0;

  return {
    play(base64: string, generation: number = 0) {
      // ì´ì „ ì„¸ëŒ€ì˜ ì˜¤ë””ì˜¤ëŠ” ë¬´ì‹œ
      if (generation < currentGeneration) {
        console.log('Ignoring old TTS chunk');
        return;
      }

      if (!audioContext) {
        audioContext = new AudioContext({ sampleRate: TTS_SAMPLE_RATE });
      }
      if (audioContext.state === 'suspended') audioContext.resume();

      const binary = atob(base64);
      const bytes = new Uint8Array(binary.length);
      for (let i = 0; i < binary.length; i++) bytes[i] = binary.charCodeAt(i);

      const view = new DataView(bytes.buffer);
      const numSamples = bytes.length / 2;
      const audioBuffer = audioContext.createBuffer(1, numSamples, TTS_SAMPLE_RATE);
      const channel = audioBuffer.getChannelData(0);
      for (let i = 0; i < numSamples; i++) {
        channel[i] = view.getInt16(i * 2, true) / 32768;
      }

      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(audioContext.destination);

      if (nextPlayTime < audioContext.currentTime) nextPlayTime = audioContext.currentTime;
      source.start(nextPlayTime);
      nextPlayTime += audioBuffer.duration;
    },

    stop() {
      nextPlayTime = 0;
    },

    updateGeneration(generation: number) {
      currentGeneration = generation;
      nextPlayTime = 0; // ì¬ìƒ í ì´ˆê¸°í™”
    }
  };
}
```

### 7.3 ì‹¤ì‹œê°„ ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬

```typescript
type SessionState = 'disconnected' | 'listening' | 'processing' | 'speaking';

export function createVoiceSession() {
  let ws: WebSocket | null = null;
  let state: SessionState = 'disconnected';
  const capture = createAudioCapture();
  const playback = createAudioPlayback();
  let onStateChange: ((state: SessionState) => void) | null = null;

  function setState(newState: SessionState) {
    state = newState;
    onStateChange?.(state);
  }

  return {
    connect(onEvent: (event: ServerEvent) => void, stateCallback: (state: SessionState) => void) {
      onStateChange = stateCallback;
      const protocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
      ws = new WebSocket(`${protocol}//${location.host}/ws`);
      ws.binaryType = 'arraybuffer';

      ws.onopen = async () => {
        setState('listening');
        // ë§ˆì´í¬ ìë™ ì‹œì‘
        try {
          await capture.start((chunk) => {
            if (ws?.readyState === WebSocket.OPEN) ws.send(chunk);
          });
        } catch (err) {
          console.error('Microphone access denied:', err);
        }
      };

      ws.onmessage = (e) => {
        const event: ServerEvent = JSON.parse(e.data);
        onEvent(event);

        switch (event.type) {
          case 'stt_output':
            // ë°œí™” ì¢…ë£Œ ê°ì§€ - ìë™ìœ¼ë¡œ LLM ì²˜ë¦¬ ìš”ì²­
            ws?.send(JSON.stringify({ type: 'auto_end_of_speech' }));
            setState('processing');
            break;

          case 'tts_chunk':
            // TTS ì¬ìƒ ì‹œì‘
            if (state !== 'speaking') setState('speaking');
            playback.play(event.audio, event.tts_generation);
            break;

          case 'tts_end':
            setState('listening');
            break;

          case 'interrupted':
            // ì¤‘ë‹¨ í™•ì¸ - ìƒì„± ë²ˆí˜¸ ì—…ë°ì´íŠ¸
            playback.updateGeneration(event.tts_generation);
            setState('listening');
            break;

          case 'error':
            setState('listening');
            break;
        }
      };

      ws.onerror = () => setState('disconnected');
      ws.onclose = () => {
        capture.stop();
        setState('disconnected');
      };
    },

    interrupt() {
      // AI ì‘ë‹µ ì¤‘ë‹¨
      if (state === 'speaking' && ws?.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({ type: 'interrupt' }));
      }
    },

    disconnect() {
      capture.stop();
      playback.stop();
      ws?.close();
      ws = null;
      setState('disconnected');
    },

    getState: () => state
  };
}
```

### 7.4 React ì»´í¬ë„ŒíŠ¸ ì˜ˆì‹œ

```tsx
import { useState, useRef, useEffect } from 'react';
import { createVoiceSession, type ServerEvent, type SessionState } from './voice';

export function VoiceChat() {
  const [state, setState] = useState<SessionState>('disconnected');
  const [transcript, setTranscript] = useState('');
  const [response, setResponse] = useState('');
  const sessionRef = useRef(createVoiceSession());

  useEffect(() => {
    const session = sessionRef.current;
    session.connect(
      (event) => {
        switch (event.type) {
          case 'stt_chunk':
            setTranscript(event.transcript);
            break;
          case 'stt_output':
            setTranscript(event.transcript);
            setResponse('');
            break;
          case 'llm_chunk':
            setResponse(prev => prev + event.text);
            break;
          case 'error':
            alert(event.message);
            break;
        }
      },
      setState
    );
    return () => session.disconnect();
  }, []);

  const handleInterrupt = () => {
    sessionRef.current.interrupt();
  };

  return (
    <div>
      <div>
        ìƒíƒœ: {state === 'listening' && 'ğŸ¤ ë“£ëŠ” ì¤‘'}
        {state === 'processing' && 'â³ ìƒê°í•˜ëŠ” ì¤‘'}
        {state === 'speaking' && 'ğŸ”Š ë§í•˜ëŠ” ì¤‘'}
        {state === 'disconnected' && 'âŒ ì—°ê²° ëŠê¹€'}
      </div>
      {state === 'speaking' && (
        <button onClick={handleInterrupt}>ì¤‘ë‹¨</button>
      )}
      <p><strong>ë‚˜:</strong> {transcript}</p>
      <p><strong>AI:</strong> {response}</p>
    </div>
  );
}
```

---

## 8. Rails ì„œë²„ êµ¬í˜„ ê°€ì´ë“œ

### 8.1 Gemfile

```ruby
gem 'faye-websocket'          # Raw WebSocket ì§€ì›
gem 'websocket-client-simple' # ì™¸ë¶€ WebSocket ì—°ê²°ìš©
gem 'google-gemini-ai'        # Gemini LLM
gem 'puma'                    # Rack hijack ì§€ì› í•„ìš”
```

### 8.2 Middleware (Raw WebSocket)

Action Cable ëŒ€ì‹  `faye-websocket`ì„ ì‚¬ìš©í•˜ì—¬ raw WebSocketì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```ruby
# lib/voice_websocket_middleware.rb
require 'faye/websocket'

class VoiceWebsocketMiddleware
  def initialize(app)
    @app = app
  end

  def call(env)
    if Faye::WebSocket.websocket?(env) && env['PATH_INFO'] == '/ws'
      ws = Faye::WebSocket.new(env)
      session = VoiceSession.new(ws)

      ws.on :open do |_|
        session.start
      end

      ws.on :message do |event|
        session.handle_message(event.data)
      end

      ws.on :close do |_|
        session.stop
      end

      ws.rack_response
    else
      @app.call(env)
    end
  end
end
```

### 8.3 VoiceSession í´ë˜ìŠ¤ (ì‹¤ì‹œê°„ ëŒ€í™”)

```ruby
# app/services/voice_session.rb
class VoiceSession
  def initialize(ws)
    @ws = ws
    @stt = nil
    @tts = nil
    @llm = nil
    @messages = []
    @current_transcript = ""
    @mutex = Mutex.new
    @processing = false
    @tts_generation = 0
  end

  def start
    @tts = CartesiaClient.new
    @llm = LlmService.new

    # Setup TTS event listeners with generation tracking
    @tts.on_event do |event|
      event_with_gen = event.merge(tts_generation: @tts_generation)
      send_event(event_with_gen)
    end

    # Connect STT persistently for full-duplex conversation
    connect_stt

    # Send initial greeting
    send_initial_greeting
  end

  def handle_message(data)
    if data.is_a?(Array) || data.encoding == Encoding::BINARY
      # Binary audio data - send to STT
      @stt&.send_audio(data)
    else
      # JSON message
      begin
        msg = JSON.parse(data)
        case msg["type"]
        when "auto_end_of_speech"
          handle_auto_end_of_speech
        when "interrupt"
          handle_interrupt
        end
      rescue JSON::ParserError
        # Treat as binary if JSON parsing fails
        @stt&.send_audio(data)
      end
    end
  end

  def stop
    disconnect_stt
    @tts&.close
    @tts = nil
    @llm = nil
  end

  private

  def connect_stt
    return if @stt

    Rails.logger.info "[VoiceSession] Connecting to STT (persistent)"
    @stt = AssemblyAiClient.new(sample_rate: 16000)

    # Setup STT event listeners
    @stt.on_event do |event|
      send_event(event)
      if event[:type] == "stt_output"
        @current_transcript = event[:transcript]
      end
    end
  end

  def disconnect_stt
    return unless @stt

    Rails.logger.info "[VoiceSession] Disconnecting STT"
    @stt.close
    @stt = nil
    @current_transcript = ""
  end

  def handle_auto_end_of_speech
    # Prevent concurrent processing
    return if @processing

    @mutex.synchronize do
      return if @processing
      @processing = true
    end

    # Force STT to finalize current turn
    @stt&.force_endpoint

    # Wait briefly for final transcript, then process
    Thread.new do
      begin
        sleep 0.5

        transcript = @current_transcript
        @current_transcript = ""

        if transcript.present?
          process_llm(transcript)
        else
          Rails.logger.warn "[VoiceSession] Empty transcript, skipping LLM processing"
        end
      ensure
        @mutex.synchronize { @processing = false }
      end
    end
  end

  def handle_interrupt
    Rails.logger.info "[VoiceSession] Interrupt received, canceling TTS"

    # Increment TTS generation to invalidate old chunks
    @mutex.synchronize do
      @tts_generation += 1
      @processing = false
    end

    # Cancel current TTS generation
    @tts&.cancel_current

    # Send interrupted acknowledgment with new generation
    send_event(type: "interrupted", tts_generation: @tts_generation)
  end

  def send_initial_greeting
    greeting = "Hello! I'm your AI English tutor. How can I help you practice English today?"

    # Send greeting text to client
    send_event(type: "llm_chunk", text: greeting)
    send_event(type: "llm_end")

    # Generate TTS for greeting (wait for connection first)
    Thread.new do
      if @tts.wait_for_connection(timeout: 5)
        @tts.send_text(greeting)
        Rails.logger.info "[VoiceSession] Sent initial greeting to TTS"
      else
        Rails.logger.error "[VoiceSession] TTS connection timeout, greeting audio not sent"
      end
    end

    # Add to conversation history
    @llm.add_message("assistant", greeting)
  end

  def process_llm(transcript)
    return if transcript.blank?

    assistant_response = ""

    # Stream LLM response (text only, no TTS yet)
    @llm.stream_response(transcript) do |text_chunk|
      assistant_response += text_chunk
      send_event(type: "llm_chunk", text: text_chunk)
    end

    # Add to conversation history
    @llm.add_message("assistant", assistant_response)

    # Send llm_end event
    send_event(type: "llm_end")

    # Generate TTS for complete response
    if assistant_response.present?
      # Increment TTS generation for new response (enables client-side buffering)
      @mutex.synchronize { @tts_generation += 1 }
      Rails.logger.info "[VoiceSession] Starting TTS generation #{@tts_generation}"

      @tts.send_text(assistant_response)
      Rails.logger.info "[VoiceSession] Sent complete response to TTS: #{assistant_response[0..50]}..."
    end
  rescue => e
    Rails.logger.error "[VoiceSession] Error processing LLM: #{e.message}"
    Rails.logger.error e.backtrace.join("\n")
    send_event(type: "error", message: "Failed to process your message. Please try again.")
  end

  def send_event(event)
    return unless @ws

    event_with_ts = { ts: (Time.now.to_f * 1000).to_i }.merge(event)
    @ws.send(event_with_ts.to_json)
  rescue => e
    Rails.logger.error "[VoiceSession] Error sending event: #{e.message}"
  end
end
```

### 8.4 AssemblyAI í´ë¼ì´ì–¸íŠ¸ (STT)

**API ë²„ì „:** v3  
**ì—”ë“œí¬ì¸íŠ¸:** `wss://streaming.assemblyai.com/v3/ws`

```ruby
# app/services/assembly_ai_client.rb
require 'websocket-client-simple'

class AssemblyAIClient
  URL = "wss://streaming.assemblyai.com/v3/ws"

  def initialize(sample_rate: 16000)
    @callbacks = []
    @mutex = Mutex.new
    connect(sample_rate)
  end

  def send_audio(bytes)
    return unless @ws&.open?
    @ws.send(bytes, type: :binary)
  end

  # AssemblyAIì— ê°•ì œ ì—”ë“œí¬ì¸íŠ¸ ì‹ í˜¸ ì „ì†¡
  def force_endpoint
    return unless @ws&.open?
    @ws.send({ type: 'force_endpoint' }.to_json)
  end

  def on_event(&block)
    @mutex.synchronize { @callbacks << block }
  end

  def close
    @ws&.close
  end

  private

  def connect(sample_rate)
    params = URI.encode_www_form(
      sample_rate: sample_rate,
      format_turns: true
    )

    @ws = WebSocket::Client::Simple.connect(
      "#{URL}?#{params}",
      headers: { 'Authorization' => ENV['ASSEMBLYAI_API_KEY'] }
    )

    @ws.on(:message) do |msg|
      handle(JSON.parse(msg.data))
    rescue JSON::ParserError => e
      Rails.logger.error("AssemblyAI parse error: #{e}")
    end

    @ws.on(:error) { |e| Rails.logger.error("AssemblyAI error: #{e}") }
  end

  def handle(data)
    return unless data['type'] == 'Turn'

    event = if data['turn_is_formatted']
      { type: 'stt_output', transcript: data['transcript'] }
    else
      { type: 'stt_chunk', transcript: data['transcript'] }
    end

    return if event[:transcript].blank?
    @mutex.synchronize { @callbacks.each { |cb| cb.call(event) } }
  end
end
```

**AssemblyAI í´ë¼ì´ì–¸íŠ¸ ë©”ì‹œì§€:**

| ë©”ì‹œì§€ | ì„¤ëª… |
|--------|------|
| `{ type: "force_endpoint" }` | í˜„ì¬ ë°œí™” ê°•ì œ ì¢…ë£Œ |
| `{ type: "terminate_session" }` | ì„¸ì…˜ ì¢…ë£Œ |

### 8.5 Cartesia í´ë¼ì´ì–¸íŠ¸ (TTS with Interrupt Support)

**API ë²„ì „:** 2025-04-16
**ëª¨ë¸:** sonic-3

```ruby
# app/services/cartesia_client.rb
require 'websocket-client-simple'

class CartesiaClient
  URL = "wss://api.cartesia.ai/tts/websocket"
  MODEL = "sonic-3"
  VOICE_ID = ENV.fetch('CARTESIA_VOICE_ID', 'f6ff7c0c-e396-40a9-a70b-f7607edb6937')
  VERSION = "2025-04-16"

  def initialize
    @callbacks = []
    @context_counter = 0
    @current_context_id = nil
    @mutex = Mutex.new
    @connected = false
    @connection_cv = ConditionVariable.new
    connect
  end

  def send_text(text)
    return unless @ws&.open? && text.present?

    @mutex.synchronize do
      @context_counter += 1
      @current_context_id = "ctx_#{(Time.now.to_f * 1000).to_i}_#{@context_counter}"
    end

    @ws.send({
      model_id: MODEL,
      transcript: text,
      voice: { mode: 'id', id: VOICE_ID },
      context_id: @current_context_id,
      output_format: { container: 'raw', encoding: 'pcm_s16le', sample_rate: 24000 },
      language: 'en'
    }.to_json)

    Rails.logger.info "[Cartesia] Sent text for TTS: #{text[0..50]}..."
  end

  def cancel_current
    return unless @ws&.open? || !@current_context_id

    # Send cancel message for current context
    @ws.send({
      context_id: @current_context_id,
      cancel: true
    }.to_json)

    Rails.logger.info "[Cartesia] Cancelled context: #{@current_context_id}"
    @current_context_id = nil
  end

  def wait_for_connection(timeout: 5)
    @mutex.synchronize do
      return true if @connected
      @connection_cv.wait(@mutex, timeout)
      @connected
    end
  end

  def on_event(&block)
    @mutex.synchronize { @callbacks << block }
  end

  def close
    @ws&.close
  end

  private

  def connect
    params = URI.encode_www_form(
      api_key: ENV['CARTESIA_API_KEY'],
      cartesia_version: VERSION
    )
    @ws = WebSocket::Client::Simple.connect("#{URL}?#{params}")

    @ws.on(:open) do
      Rails.logger.info "[Cartesia] Connected to TTS WebSocket"
      @mutex.synchronize do
        @connected = true
        @connection_cv.broadcast
      end
    end

    @ws.on(:message) do |msg|
      handle(JSON.parse(msg.data))
    rescue JSON::ParserError => e
      Rails.logger.error("[Cartesia] Parse error: #{e}")
    end

    @ws.on(:error) { |e| Rails.logger.error("[Cartesia] Error: #{e}") }
    @ws.on(:close) do
      Rails.logger.info "[Cartesia] Connection closed"
      @mutex.synchronize { @connected = false }
    end
  end

  def handle(data)
    if data['done']
      # TTS generation complete
      @mutex.synchronize do
        @callbacks.each { |cb| cb.call(type: 'tts_end') }
      end
    elsif data['data']
      # TTS audio chunk
      @mutex.synchronize do
        @callbacks.each { |cb| cb.call(type: 'tts_chunk', audio: data['data']) }
      end
    end
  end
end
```

### 8.6 Middleware ë“±ë¡

```ruby
# config/application.rb
require_relative '../lib/voice_websocket_middleware'

module YourApp
  class Application < Rails::Application
    config.middleware.use VoiceWebsocketMiddleware
  end
end
```

### 8.7 Puma ì„¤ì • (Rack hijack í™œì„±í™”)

```ruby
# config/puma.rb
workers 0  # WebSocketì€ ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ê¶Œì¥
threads_count = ENV.fetch("RAILS_MAX_THREADS") { 5 }
threads threads_count, threads_count
```

### 8.8 í™˜ê²½ ë³€ìˆ˜

```bash
ASSEMBLYAI_API_KEY=your_key
CARTESIA_API_KEY=your_key
CARTESIA_VOICE_ID=f6ff7c0c-e396-40a9-a70b-f7607edb6937  # Optional
GEMINI_API_KEY=your_key
```

---

## 9. ì˜¤ë””ì˜¤ í¬ë§· ìš”ì•½

| êµ¬ê°„ | ìƒ˜í”Œë ˆì´íŠ¸ | ë¹„íŠ¸ | ì±„ë„ | ì¸ì½”ë”© |
|------|-----------|------|------|--------|
| í´ë¼ì´ì–¸íŠ¸ â†’ ì„œë²„ | 16kHz | 16-bit signed | mono | raw PCM (little-endian) |
| ì„œë²„ â†’ í´ë¼ì´ì–¸íŠ¸ (TTS) | 24kHz | 16-bit signed | mono | Base64(raw PCM LE) |

---

## 10. ìƒíƒœ ë‹¤ì´ì–´ê·¸ë¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ disconnected â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ WebSocket ì—°ê²° + ë§ˆì´í¬ ìë™ ì‹œì‘
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  ë°œí™” ì¢…ë£Œ ê°ì§€   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  LLM ì‘ë‹µ ì™„ë£Œ   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  listening   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ processing â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ speaking  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       ^                                                                â”‚
       â”‚                                                                â”‚
       â”‚                          TTS ì¬ìƒ ì™„ë£Œ ë˜ëŠ” interrupt          â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ìƒíƒœ ì„¤ëª…:**
- `disconnected`: WebSocket ì—°ê²° ì „ ë˜ëŠ” ì—°ê²° ëŠê¹€
- `listening`: ë§ˆì´í¬ í™œì„±í™”, ì‚¬ìš©ì ìŒì„± ëŒ€ê¸° ì¤‘
- `processing`: ë°œí™” ì¢…ë£Œ ê°ì§€ í›„ LLM ì‘ë‹µ ìƒì„± ì¤‘
- `speaking`: AI ì‘ë‹µ TTS ì¬ìƒ ì¤‘ (interrupt ê°€ëŠ¥)

---

## 11. ì—ëŸ¬ ì²˜ë¦¬

### í´ë¼ì´ì–¸íŠ¸

```typescript
ws.onerror = () => {
  // ì¬ì—°ê²° ë˜ëŠ” ì‚¬ìš©ì ì•Œë¦¼
  setState('disconnected');
};

// ì„œë²„ ì—ëŸ¬ ì´ë²¤íŠ¸ ì²˜ë¦¬
if (event.type === 'error') {
  alert(event.message);
  setState('listening');
}
```

### ì„œë²„

```ruby
def process_llm(transcript)
  # ...
rescue => e
  Rails.logger.error "[VoiceSession] Error processing LLM: #{e.message}"
  send_event(type: 'error', message: "Failed to process your message. Please try again.")
end
```

---

## 12. ì‹¤ì‹œê°„ ëŒ€í™” êµ¬í˜„ í•µì‹¬ í¬ì¸íŠ¸

### 12.1 ì§€ì†ì  STT ì—°ê²°

- WebSocket ì—°ê²° ì‹œ STTë¥¼ ì¦‰ì‹œ ì—°ê²°í•˜ê³  ì„¸ì…˜ ì¢…ë£Œê¹Œì§€ ìœ ì§€
- ë§ˆì´í¬ë¥¼ í•­ìƒ í™œì„±í™”í•˜ì—¬ ì§€ì†ì ìœ¼ë¡œ ì˜¤ë””ì˜¤ ì „ì†¡
- AssemblyAIì˜ `format_turns` íŒŒë¼ë¯¸í„°ë¡œ ë°œí™” ì¢…ë£Œ ìë™ ê°ì§€

### 12.2 ìë™ ë°œí™” ì¢…ë£Œ ê°ì§€

- AssemblyAIê°€ `stt_output` ì´ë²¤íŠ¸ë¡œ ë°œí™” ì¢…ë£Œ ì•Œë¦¼
- í´ë¼ì´ì–¸íŠ¸ê°€ ìë™ìœ¼ë¡œ `auto_end_of_speech` ë©”ì‹œì§€ ì „ì†¡
- ì„œë²„ê°€ LLM ì²˜ë¦¬ ì‹œì‘

### 12.3 Interrupt ì²˜ë¦¬

- AI ì‘ë‹µ ì¤‘ ì‚¬ìš©ìê°€ ë§í•˜ë©´ í´ë¼ì´ì–¸íŠ¸ê°€ `interrupt` ë©”ì‹œì§€ ì „ì†¡
- ì„œë²„ê°€ TTS ìƒì„± ë²ˆí˜¸ë¥¼ ì¦ê°€ì‹œì¼œ ì´ì „ ì²­í¬ ë¬´íš¨í™”
- í´ë¼ì´ì–¸íŠ¸ê°€ ìƒì„± ë²ˆí˜¸ë¥¼ í™•ì¸í•˜ì—¬ ì´ì „ ì˜¤ë””ì˜¤ ë¬´ì‹œ

### 12.4 TTS ìƒì„± ë²ˆí˜¸ ê´€ë¦¬

```ruby
# ì„œë²„: ìƒˆ ì‘ë‹µë§ˆë‹¤ ìƒì„± ë²ˆí˜¸ ì¦ê°€
@tts_generation += 1
send_event(type: 'tts_chunk', audio: data, tts_generation: @tts_generation)

# í´ë¼ì´ì–¸íŠ¸: ì´ì „ ìƒì„± ë²ˆí˜¸ì˜ ì²­í¬ ë¬´ì‹œ
if (chunkGeneration < currentTtsGenerationRef.current) {
  console.log('Ignoring old TTS chunk');
  return;
}
```

---

## 13. í…ŒìŠ¤íŠ¸

### WebSocket ì—°ê²° í…ŒìŠ¤íŠ¸

```bash
# wscat ì„¤ì¹˜
npm install -g wscat

# ì—°ê²° í…ŒìŠ¤íŠ¸
wscat -c ws://localhost:3000/ws

# JSON ë©”ì‹œì§€ ì „ì†¡
> {"type":"auto_end_of_speech"}
> {"type":"interrupt"}
```

### ì˜¤ë””ì˜¤ íŒŒì¼ ì „ì†¡ í…ŒìŠ¤íŠ¸ (Ruby)

```ruby
require 'websocket-client-simple'

ws = WebSocket::Client::Simple.connect('ws://localhost:3000/ws')

ws.on(:message) { |msg| puts msg.data }

# PCM íŒŒì¼ ì „ì†¡ (ì§€ì†ì ìœ¼ë¡œ)
File.open('test.pcm', 'rb') do |f|
  while (chunk = f.read(3200))  # 100ms chunks
    ws.send(chunk, type: :binary)
    sleep 0.1
  end
end

# ë°œí™” ì¢…ë£Œ ì‹œë®¬ë ˆì´ì…˜
sleep 1
ws.send({ type: 'auto_end_of_speech' }.to_json)
```

---

## 14. ì„±ëŠ¥ ìµœì í™”

### 14.1 ë ˆì´í„´ì‹œ ìµœì†Œí™”

- **STT**: AssemblyAI v3ì˜ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©
- **LLM**: Gemini 2.5 Flash Liteì˜ ìŠ¤íŠ¸ë¦¬ë° API ì‚¬ìš©
- **TTS**: Cartesia sonic-3ì˜ WebSocket ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©
- **ë„¤íŠ¸ì›Œí¬**: Raw WebSocketìœ¼ë¡œ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”

### 14.2 ì˜¤ë””ì˜¤ ë²„í¼ë§

```typescript
// í´ë¼ì´ì–¸íŠ¸: Web Audio APIì˜ ìŠ¤ì¼€ì¤„ë§ìœ¼ë¡œ ëŠê¹€ ì—†ëŠ” ì¬ìƒ
if (nextPlayTime < audioContext.currentTime) {
  nextPlayTime = audioContext.currentTime;
}
source.start(nextPlayTime);
nextPlayTime += audioBuffer.duration;
```

### 14.3 ë™ì‹œì„± ì œì–´

```ruby
# ì„œë²„: Mutexë¡œ ë™ì‹œ ì²˜ë¦¬ ë°©ì§€
@mutex.synchronize do
  return if @processing
  @processing = true
end
```

---

## 15. ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### 15.1 WebSocket ì¸ì¦

```ruby
# Middlewareì—ì„œ í† í° ê²€ì¦
def call(env)
  if Faye::WebSocket.websocket?(env) && env['PATH_INFO'] == '/ws'
    # í† í° ê²€ì¦ ë¡œì§
    token = env['HTTP_AUTHORIZATION']&.sub(/^Bearer /, '')
    unless valid_token?(token)
      return [401, {}, ['Unauthorized']]
    end

    # WebSocket ì—°ê²° ì²˜ë¦¬
    # ...
  end
end
```

### 15.2 Rate Limiting

```ruby
# Redisë¥¼ ì‚¬ìš©í•œ rate limiting
def handle_message(data)
  user_id = @user_id
  key = "ws_rate_limit:#{user_id}"

  count = Redis.current.incr(key)
  Redis.current.expire(key, 60) if count == 1

  if count > 1000  # ë¶„ë‹¹ 1000 ë©”ì‹œì§€ ì œí•œ
    send_event(type: 'error', message: 'Rate limit exceeded')
    return
  end

  # ë©”ì‹œì§€ ì²˜ë¦¬
  # ...
end
```

---

## 16. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 16.1 STT ì—°ê²° ëŠê¹€

**ì¦ìƒ**: `stt_chunk` ì´ë²¤íŠ¸ê°€ ë” ì´ìƒ ìˆ˜ì‹ ë˜ì§€ ì•ŠìŒ

**í•´ê²°**:
```ruby
# ì—°ê²° ìƒíƒœ ëª¨ë‹ˆí„°ë§ ë° ì¬ì—°ê²°
def ensure_stt_connected
  return if @stt&.connected?

  Rails.logger.warn "[VoiceSession] STT disconnected, reconnecting..."
  disconnect_stt
  connect_stt
end
```

### 16.2 TTS ì˜¤ë””ì˜¤ ëŠê¹€

**ì¦ìƒ**: ì˜¤ë””ì˜¤ ì¬ìƒì´ ëŠê¸°ê±°ë‚˜ ì§€ì—°ë¨

**í•´ê²°**:
```typescript
// ë²„í¼ í¬ê¸° ì¡°ì • ë° ìŠ¤ì¼€ì¤„ë§ ê°œì„ 
const BUFFER_THRESHOLD = 0.1; // 100ms ë²„í¼
if (nextPlayTime - audioContext.currentTime < BUFFER_THRESHOLD) {
  nextPlayTime = audioContext.currentTime + BUFFER_THRESHOLD;
}
```

### 16.3 Interrupt ë¯¸ì‘ë™

**ì¦ìƒ**: interrupt ë©”ì‹œì§€ë¥¼ ë³´ë‚´ë„ TTSê°€ ê³„ì† ì¬ìƒë¨

**í•´ê²°**:
```typescript
// ìƒì„± ë²ˆí˜¸ í™•ì¸ ë¡œì§ ê°•í™”
const chunkGeneration = data.tts_generation ?? 0;
if (chunkGeneration < currentTtsGenerationRef.current) {
  console.log('[WebSocket] Ignoring old TTS chunk');
  return; // ì´ì „ ì²­í¬ ë¬´ì‹œ
}
```

---

## 17. ì°¸ê³  ìë£Œ

- [AssemblyAI v3 API Documentation](https://www.assemblyai.com/docs)
- [Cartesia TTS API Documentation](https://docs.cartesia.ai/)
- [Google Gemini API Documentation](https://ai.google.dev/docs)
- [Faye WebSocket Documentation](https://github.com/faye/faye-websocket-ruby)
- [Web Audio API Documentation](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API)
